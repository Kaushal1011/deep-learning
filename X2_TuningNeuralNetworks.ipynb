{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks \n",
    "\n",
    "## Number of Hidden layers\n",
    "\n",
    "For many problems, you can just begin with a single hidden layer and you will get\n",
    "reasonable results. It has actually been shown that an MLP with just one hidden layer\n",
    "can model even the most complex functions provided it has enough neurons.\n",
    "\n",
    "\n",
    " suppose you are asked to draw a forest using some drawing soft‐\n",
    "ware, but you are forbidden to use copy/paste. You would have to draw each tree\n",
    "individually, branch per branch, leaf per leaf. If you could instead draw one leaf,\n",
    "copy/paste it to draw a branch, then copy/paste that branch to create a tree, and\n",
    "finally copy/paste this tree to make a forest, you would be finished in no time. Realworld data is often structured in such a hierarchical way and Deep Neural Networks\n",
    "automatically take advantage of this fact: lower hidden layers model low-level struc‐\n",
    "tures (e.g., line segments of various shapes and orientations), intermediate hidden\n",
    "layers combine these low-level structures to model intermediate-level structures (e.g.,\n",
    "squares, circles), and the highest hidden layers and the output layer combine these\n",
    "intermediate structures to model high-level structures (e.g., faces).\n",
    "\n",
    "Not only does this hierarchical architecture help DNNs converge faster to a good sol‐\n",
    "ution, it also improves their ability to generalize to new datasets. For example, if you\n",
    "have already trained a model to recognize faces in pictures, and you now want to\n",
    "train a new neural network to recognize hairstyles, then you can kickstart training by\n",
    "reusing the lower layers of the first network. Instead of randomly initializing the\n",
    "weights and biases of the first few layers of the new neural network, you can initialize\n",
    "them to the value of the weights and biases of the lower layers of the first network.\n",
    "This way the network will not have to learn from scratch all the low-level structures\n",
    "that occur in most pictures; it will only have to learn the higher-level structures (e.g.,\n",
    "hairstyles). This is called transfer learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Neurons per Hidden Layer\n",
    "\n",
    "Obviously the number of neurons in the input and output layers is determined by the\n",
    "type of input and output your task requires. For example, the MNIST task requires 28\n",
    "x 28 = 784 input neurons and 10 output neurons\n",
    "\n",
    "Just like for the number of layers, you can try increasing the number of neurons grad‐\n",
    "ually until the network starts overfitting. In general you will get more bang for the\n",
    "buck by increasing the number of layers than the number of neurons per layer.\n",
    "Unfortunately, as you can see, finding the perfect amount of neurons is still somewhat\n",
    "of a dark art.\n",
    "A simpler approach is to pick a model with more layers and neurons than you\n",
    "actually need, then use early stopping to prevent it from overfitting (and other regu‐\n",
    "larization techniques, such as dropout)\n",
    "\n",
    "depending on the dataset, it can sometimes help\n",
    "to make the first hidden layer bigger than the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate, Batch Size and Other Hyperparameters\n",
    "\n",
    "- The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges).So a simple approach for tuning the learning rate is to start with a large value that makes the training algorithm diverge, then divide this value by 3 and try again, and repeat until the training algorithm stops diverging. At that point, you generally won’t be too far from the optimal learning rate.\n",
    "- Choosing a better optimizer than plain old Mini-batch Gradient Descent (and tuning its hyperparameters) is also quite important.\n",
    "- The batch size can also have a significant impact on your model’s performance and the training time. In general the optimal batch size will be lower than 32.\n",
    "- in general, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on your task.\n",
    "- the number of training iterations does not actually need to be tweaked: just use early stopping instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
