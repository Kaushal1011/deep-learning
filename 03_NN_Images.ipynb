{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to convnets\n",
    "\n",
    "### The conolution operation\n",
    "\n",
    "The fundamental difference between a densely connected layer and a convolution\n",
    "layer is this: Dense layers learn global patterns in their input feature space (for example, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn\n",
    "local patterns (see figure 5.1): in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 × 3.\n",
    "\n",
    "convnets two interesting properties:\n",
    "\n",
    "- The patterns they learn are translation invariant. After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn the pattern anew if it appeared at a new location. This makes convnets data efficient when processing images (because the visual world is fundamentallytranslation invariant): they need fewer training samples to learn representations that have generalization power\n",
    "- They can learn spatial hierarchies of patterns (see figure 5.2). A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because the visual world is fundamentally spatially hierarchical).\n",
    "\n",
    "Cat Image example where nn would first find small pattern such as edges and shapes and then it would combine it to make better visuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height\n",
    "and width) as well as a depth axis (also called the channels axis).\n",
    "\n",
    "The convolution operation extracts patches from its input feature\n",
    "map and applies the same transformation to all of these patches, producing an output\n",
    "feature map. This output feature map is still a 3D tensor: it has a width and a height. Its\n",
    "depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB\n",
    "input; rather, they stand for filters. Filters encode specific aspects of the input data: at a\n",
    "high level, a single filter could encode the concept “presence of a face in the input,”\n",
    "for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MNIST example, the first convolution layer takes a feature map of size (28,\n",
    "28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its\n",
    "input. Each of these 32 output channels contains a 26 × 26 grid of values, which is a\n",
    "response map of the filter over the input, indicating the response of that filter pattern at\n",
    "different locations in the input (see figure 5.3). That is what the term feature map\n",
    "means: every dimension in the depth axis is a feature (or filter), and the 2D tensor\n",
    "output[:, :, n] is the 2D spatial map of the response of this filter over the input.\n",
    "\n",
    "Convolutions are defined by two key parameters:\n",
    "- Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the example, they were 3 × 3, which is a common choice.\n",
    "- Depth of the output feature map—The number of filters computed by the convolution. The example started with a depth of 32 and ended with a depth of 64.\n",
    "\n",
    "Conv2D(output_depth, (window_height, window_width)).\n",
    " A convolution works by sliding these windows of size 3 × 3 or 5 × 5 over the 3D input\n",
    "feature map, stopping at every possible location, and extracting the 3D patch of surrounding features (shape (window_height, window_width, input_depth)). Each\n",
    "such 3D patch is then transformed (via a tensor product with the same learned weight\n",
    "matrix, called the convolution kernel) into a 1D vector of shape (output_depth,). All of\n",
    "these vectors are then spatially reassembled into a 3D output map of shape (height,\n",
    "width, output_depth). Every spatial location in the output feature map corresponds\n",
    "to the same location in the input feature map (for example, the lower-right corner of\n",
    "the output contains information about the lower-right corner of the input). For\n",
    "instance, with 3 × 3 windows, the vector output[i, j, :] comes from the 3D patch\n",
    "input[i-1:i+1, j-1:j+1, :]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![con](img/convwork.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output width and height may differ from the input width and height.\n",
    "They may differ for two reasons:\n",
    "- Border effects, which can be countered by padding the input feature map\n",
    "- The use of strides, which I’ll define in a second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UNDERSTANDING BORDER EFFECTS AND PADDING\n",
    "- UNDERSTANDING CONVOLUTION STRIDES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The max-pooling operation\n",
    "\n",
    "Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It’s conceptually similar to convolution, except\n",
    "that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand,\n",
    "convolution is typically done with 3 × 3 windows and no stride (stride 1).\n",
    "\n",
    "![](img/maxpool.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the reason to use downsampling is to reduce the number of feature-map\n",
    "coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of\n",
    "the original input they cover).\n",
    "\n",
    "Other ways of downsampling:\n",
    "- striding in convolution layer\n",
    "- using average pool instead of max pooling\n",
    "\n",
    "Max pooling tends to work better than these alternative solutions. In a nutshell, the reason is that features tend to encode the spatial presence of some pattern\n",
    "or concept over the different tiles of the feature map (hence, the term feature map),\n",
    "and it’s more informative to look at the maximal presence of different features than at\n",
    "their average presence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnet from scratch \n",
    "\n",
    "- The relevance of deep learning for small-data problems\n",
    "- Dogs vs Cats Dataset \n",
    "    -  This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\n",
    "- Building your network\n",
    "    - the convnet will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers\n",
    "    ![](img/scratchconvnet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data preprocessing\n",
    "    1. Read the picture files.\n",
    "    2. Decode the JPEG content to RGB grids of pixels.\n",
    "    3. Convert these into floating-point tensors.\n",
    "    4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using data augmentation\n",
    "    - Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "![](img/dataaugkeras.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you train a new network using this data-augmentation configuration, the network\n",
    "will never see the same input twice. But the inputs it sees are still heavily intercorrelated, because they come from a small number of original images—you can’t produce new information, you can only remix existing information. As such, this may not\n",
    "be enough to completely get rid of overfitting. To further fight overfitting, you’ll also\n",
    "add a Dropout layer to your model, right before the densely connected classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained convnet\n",
    "\n",
    "A common and highly effective approach to deep learning on small image datasets is\n",
    "to use a pretrained network. A pretrained network is a saved network that was previously\n",
    "trained on a large dataset, typically on a large-scale image-classification task. If this\n",
    "original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the\n",
    "visual world, and hence its features can prove useful for many different computervision problems, even though these new problems may involve completely different\n",
    "classes than those of the original task\n",
    "\n",
    "\n",
    " Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it\n",
    "makes deep learning very effective for small-data problems.\n",
    "\n",
    "\n",
    "There are two ways to use a pretrained network:\n",
    "- feature extraction\n",
    "- fine-tuning\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "Feature extraction consists of using the representations learned by a previous network\n",
    "to extract interesting features from new samples. These features are then run through\n",
    "a new classifier, which is trained from scratch.\n",
    "\n",
    "![](img/featextraction.PNG)\n",
    "\n",
    "Why only reuse the convolutional base? Could you reuse the densely connected classifier as well? In general, doing so should be avoided. The reason is that the representations learned by the convolutional base are likely to be more generic and therefore\n",
    "more reusable: the feature maps of a convnet are presence maps of generic concepts\n",
    "over a picture, which is likely to be useful regardless of the computer-vision problem at\n",
    "hand. But the representations learned by the classifier will necessarily be specific to the\n",
    "set of classes on which the model was trained—they will only contain information about\n",
    "the presence probability of this or that class in the entire picture. Additionally, representations found in densely connected layers no longer contain any information about\n",
    "where objects are located in the input image: these layers get rid of the notion of space,\n",
    "whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely connected features are largely useless.\n",
    "\n",
    "Here’s the list of image-classification\n",
    "models (all pretrained on the ImageNet dataset) that are available as part of keras\n",
    ".applications:\n",
    "- Xception\n",
    "- Inception V3\n",
    "- ResNet50\n",
    "- VGG16\n",
    "- VGG19\n",
    "- MobileNet\n",
    "\n",
    "#### Fast Feature Extraction without Data Augmentation\n",
    "\n",
    "#### Feature Extraction with Data Augmentation\n",
    "\n",
    "### Fine-tuning\n",
    "\n",
    " Fine-tuning consists of unfreezing a few of\n",
    "the top layers of a frozen model base used for feature extraction, and jointly training\n",
    "both the newly added part of the model (in this case, the fully connected classifier)\n",
    "and these top layers. This is called fine-tuning because it slightly adjusts the more\n",
    "abstract representations of the model being reused, in order to make them more relevant for the problem at hand.\n",
    "\n",
    "![](img/finetune.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> it’s necessary to freeze the convolution base of VGG16 in order to\n",
    "be able to train a randomly initialized classifier on top. For the same reason, it’s only\n",
    "possible to fine-tune the top layers of the convolutional base once the classifier on top\n",
    "has already been trained. If the classifier isn’t already trained, then the error signal\n",
    "propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed.\n",
    "\n",
    "steps for fine-tuning a network are as follow:\n",
    "1. Add your custom network on top of an already-trained base network.\n",
    "2. Freeze the base network.\n",
    "3. Train the part you added.\n",
    "4. Unfreeze some layers in the base network.\n",
    "5. Jointly train both these layers and the part you added\n",
    "\n",
    "\n",
    "Why not fine-tune more layers? Why not fine-tune the entire convolutional base?\n",
    "You could. But you need to consider the following:\n",
    "- Earlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features. It’s more useful to fine-tune the more specialized features, because these are the ones that need to be repurposed on your new problem. There would be fast-decreasing returns in fine-tuning lower layers.\n",
    "- The more parameters you’re training, the more you’re at risk of overfitting. The convolutional base has 15 million parameters, so it would be risky to attempt to train it on your small dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "- Convnets are the best type of machine-learning models for computer-vision tasks. It’s possible to train one from scratch even on a very small dataset, with decent results.\n",
    "- On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you’re working with image data.\n",
    "- It’s easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.\n",
    "- As a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
