{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to convnets\n",
    "\n",
    "### The conolution operation\n",
    "\n",
    "The fundamental difference between a densely connected layer and a convolution\n",
    "layer is this: Dense layers learn global patterns in their input feature space (for example, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn\n",
    "local patterns (see figure 5.1): in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 × 3.\n",
    "\n",
    "convnets two interesting properties:\n",
    "\n",
    "- The patterns they learn are translation invariant. After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn the pattern anew if it appeared at a new location. This makes convnets data efficient when processing images (because the visual world is fundamentallytranslation invariant): they need fewer training samples to learn representations that have generalization power\n",
    "- They can learn spatial hierarchies of patterns (see figure 5.2). A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because the visual world is fundamentally spatially hierarchical).\n",
    "\n",
    "Cat Image example where nn would first find small pattern such as edges and shapes and then it would combine it to make better visuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions operate over 3D tensors, called feature maps, with two spatial axes (height\n",
    "and width) as well as a depth axis (also called the channels axis).\n",
    "\n",
    "The convolution operation extracts patches from its input feature\n",
    "map and applies the same transformation to all of these patches, producing an output\n",
    "feature map. This output feature map is still a 3D tensor: it has a width and a height. Its\n",
    "depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB\n",
    "input; rather, they stand for filters. Filters encode specific aspects of the input data: at a\n",
    "high level, a single filter could encode the concept “presence of a face in the input,”\n",
    "for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MNIST example, the first convolution layer takes a feature map of size (28,\n",
    "28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its\n",
    "input. Each of these 32 output channels contains a 26 × 26 grid of values, which is a\n",
    "response map of the filter over the input, indicating the response of that filter pattern at\n",
    "different locations in the input (see figure 5.3). That is what the term feature map\n",
    "means: every dimension in the depth axis is a feature (or filter), and the 2D tensor\n",
    "output[:, :, n] is the 2D spatial map of the response of this filter over the input.\n",
    "\n",
    "Convolutions are defined by two key parameters:\n",
    "- Size of the patches extracted from the inputs—These are typically 3 × 3 or 5 × 5. In the example, they were 3 × 3, which is a common choice.\n",
    "- Depth of the output feature map—The number of filters computed by the convolution. The example started with a depth of 32 and ended with a depth of 64.\n",
    "\n",
    "Conv2D(output_depth, (window_height, window_width)).\n",
    " A convolution works by sliding these windows of size 3 × 3 or 5 × 5 over the 3D input\n",
    "feature map, stopping at every possible location, and extracting the 3D patch of surrounding features (shape (window_height, window_width, input_depth)). Each\n",
    "such 3D patch is then transformed (via a tensor product with the same learned weight\n",
    "matrix, called the convolution kernel) into a 1D vector of shape (output_depth,). All of\n",
    "these vectors are then spatially reassembled into a 3D output map of shape (height,\n",
    "width, output_depth). Every spatial location in the output feature map corresponds\n",
    "to the same location in the input feature map (for example, the lower-right corner of\n",
    "the output contains information about the lower-right corner of the input). For\n",
    "instance, with 3 × 3 windows, the vector output[i, j, :] comes from the 3D patch\n",
    "input[i-1:i+1, j-1:j+1, :]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![con](img/convwork.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output width and height may differ from the input width and height.\n",
    "They may differ for two reasons:\n",
    "- Border effects, which can be countered by padding the input feature map\n",
    "- The use of strides, which I’ll define in a second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UNDERSTANDING BORDER EFFECTS AND PADDING\n",
    "- UNDERSTANDING CONVOLUTION STRIDES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The max-pooling operation\n",
    "\n",
    "Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It’s conceptually similar to convolution, except\n",
    "that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation. A big difference from convolution is that max pooling is usually done with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand,\n",
    "convolution is typically done with 3 × 3 windows and no stride (stride 1).\n",
    "\n",
    "![](img/maxpool.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the reason to use downsampling is to reduce the number of feature-map\n",
    "coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of\n",
    "the original input they cover).\n",
    "\n",
    "Other ways of downsampling:\n",
    "- striding in convolution layer\n",
    "- using average pool instead of max pooling\n",
    "\n",
    "Max pooling tends to work better than these alternative solutions. In a nutshell, the reason is that features tend to encode the spatial presence of some pattern\n",
    "or concept over the different tiles of the feature map (hence, the term feature map),\n",
    "and it’s more informative to look at the maximal presence of different features than at\n",
    "their average presence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convnet from scratch \n",
    "\n",
    "- The relevance of deep learning for small-data problems\n",
    "- Dogs vs Cats Dataset \n",
    "    -  This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\n",
    "- Building your network\n",
    "    - the convnet will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers\n",
    "    ![](img/scratchconvnet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data preprocessing\n",
    "    1. Read the picture files.\n",
    "    2. Decode the JPEG content to RGB grids of pixels.\n",
    "    3. Convert these into floating-point tensors.\n",
    "    4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval (as you know, neural networks prefer to deal with small input values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using data augmentation\n",
    "    - Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "![](img/dataaugkeras.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you train a new network using this data-augmentation configuration, the network\n",
    "will never see the same input twice. But the inputs it sees are still heavily intercorrelated, because they come from a small number of original images—you can’t produce new information, you can only remix existing information. As such, this may not\n",
    "be enough to completely get rid of overfitting. To further fight overfitting, you’ll also\n",
    "add a Dropout layer to your model, right before the densely connected classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained convnet\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
